{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing historical demand and predicting sales using Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capstone project for Intermediate Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sumithra Candasamy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the sales based on historical demand and holiday markdown events for 45 stores located in different regions. Each store contains many departments and there is a need to project the sales for each department in each store. Markdowns affect sales, and the challenge is to predict which departments are affected and the extent of the impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project investigates the application data sets and creates models that predict the sales for future timeframe. Based on this information, the client will be able to make better inventory handling decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we have for this project is taken from the Kaggle website. The stores information, training and test data and features data is available to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  _stores.csv_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This file contains anonymized information about the 45 stores, indicating the type and size of store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  _train.csv_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is the historical training data, which covers to 2010-02-05 to 2012-11-01. Within this file you will find the following fields\n",
    "- the store number\n",
    "- the department number\n",
    "- the weekly date\n",
    "-  sales for the given department in the given store\n",
    "- whether the week is a special holiday week\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This file is identical to train.csv, except we have withheld the weekly sales. You must predict the sales for each triplet of store, department, and date in this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- features.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This file contains additional data related to the store, department, and regional activity for the given dates. It contains the following fields:\n",
    "- the store number\n",
    "- the week\n",
    "- average temperature in the region\n",
    "- cost of fuel in the region\n",
    "- MarkDown1-5 - anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.\n",
    "- CPI - the consumer price index\n",
    "- Unemployment - the unemployment rate\n",
    "- IsHoliday - whether the week is a special holiday week\n",
    "> For convenience, the four holidays fall within the following weeks in the dataset (not all holidays are in the data):\n",
    "> Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n",
    "> Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n",
    "> Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n",
    "> Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used the forecasting procedures (FBProphet and ARIMA) that are used to forecast time series data.\n",
    "\n",
    "**Prophet** is a procedure for forecasting time series data. It is based on an additive model where non-linear trends are fit with yearly and weekly seasonality, plus holidays. It works best with daily periodicity data with at least one year of historical data. Prophet is robust to missing data, shifts in the trend, and large outliers.\n",
    "\n",
    "In time series analysis, an autoregressive integrated moving average (**ARIMA**) model is a generalization of an autoregressive moving average (ARMA) model. Both ARMA and ARIMA models are fitted to time series data either to better understand the data or to predict future points in the series (forecasting). ARIMA models are applied in some cases where data show evidence of non-stationarity, where an initial differencing step (corresponding to the \"integrated\" part of the model) can be applied one or more times to eliminate the non-stationarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas for :\n",
    "-  data loading, wrangling, cleaning and manipulation\n",
    "-  feature selection and engineering\n",
    "-  description statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy for:\n",
    "- array data structure, matrix manipulation    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matplotlib for:\n",
    "- data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn for:\n",
    "- model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in the form of CSV files, which are easily loaded into pandas dataframes. All the data is read as their respective datatype (int, float, bool), and the dates are parsed as dates automatically. Some of weeks are missing in the train csv, and these are filled with 0 for sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection and Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset contains the historical sales for 45 stores in 81 departments, I have used this information, along with the holidays. This showed the seasonality of the sales in different store/department combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prophet model is used with holidays information and yearly seasonality. The data was split into train and test sets, and the model was applied to the train set. The model was then used to define future timeframes to include the length of test set. The predict method is used to forecast for the future timeframe.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three distinct integers (p, d, q) that are used to parametrize ARIMA models. These three parameters account for seasonality, trend, and noise in datasets. \n",
    "\n",
    "p is the auto-regressive part of the model. It allows us to incorporate the effect of past values into our model.\n",
    "d is the integrated part of the model. This includes terms in the model that incorporate the amount of differencing.\n",
    "q is the moving average part of the model. This allows us to set the error of our model as a linear combination of the error values observed at previous time points in the past.\n",
    "\n",
    "The Augmented Dickey Fuller test is run to check stationarity of time series data. \n",
    "\n",
    "The Autocorrelation and Partial Autocorrelation plots are plotted on the data to get a range for the p and q parameters. The range is then used to find the optimal parameters for p and q using grid search.\n",
    "\n",
    "Since we have time series with seasonal effects, we can use the seasonal ARIMA, which is denoted as ARIMA(p,d,q)(P,D,Q)s. Here, (p, d, q) are the non-seasonal parameters, while (P, D, Q) follow the same definition but are applied to the seasonal component of the time series. \n",
    "The term s is the periodicity of the time series (4 for quarterly periods, 12 for yearly periods, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE and MAE is captured for both the models for all stores and departments. It is observed that the RMSE and MAE of the Prophet forecast model are smaller when compared to the ARIMA model. So the Prophet forecasting model appears to be more accurate than the ARIMA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of Prophet model is overall better compared to the ARIMA model. However some departments in Prophet with model show a relatively high MAE, namely departments 38, 72. It would be better to consult the product manager to get more information on these departments.\n",
    "Also the Prophet model is better in evaluating with store/depts having 0 sales for certain weeks, while the ARIMA model does not produce good pdq's to create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
